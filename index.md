---
layout: home
title: "Qin Zhang"
---

# Biography
---

She is currently an Assistant Professor with the [College of Computer Science and Software Engineering](https://csse.szu.edu.cn/pages/user/index?id=1309), [Shenzhen University](https://www.szu.edu.cn/), China. Before joining Shenzhen University, she was a Senior Researcher with [Tencent Technology Co., Ltd.](https://www.tencent.com/) from 2018 to 2021. She received her Ph.D degree in computer science from [University of Technology Sydney (UTS)](https://www.uts.edu.au/), Australia (2018), supervised by [Prof. Chengqi Zhang](https://profiles.uts.edu.au/Chengqi.Zhang), [A.P. Guodong Long](https://profiles.uts.edu.au/Guodong.Long) and Prof. Peng Zhang, and got her master’s degree from [University of Chinese Academy of Science (UCAS)](https://www.ucas.ac.cn/), Beijing, China, supervised by [Prof. Yingjie Tian](https://people.ucas.ac.cn/~tianyingjie?language=en). During September 2017 to March 2018, She was fortunate to visit [Prof. Wei Wang](https://web.cs.ucla.edu/~weiwang/) with Department of [Computer Science](https://www.cs.ucla.edu/) at [University of California, Los Angeles (UCLA)](https://www.ucla.edu/).

In the past few years, she has been recognized as Overseas High-Caliber Personnel (深圳市海外高层次人才), High-level Talent in Nanshan District of Shenzhen (南山区领航人才).

Her research interests focus on **graph learning**, **natural language processing**, and **open-world machine learning**, etc. She have made contributions to advance graph machine learning methods for solving hard open-world AI problems for real-life applications, including node classification, question answering, anomaly detection, recommender systems and time series clustering. She has published more than 40 research papers in top journals and conferences, including **TPAMI, TKDE, TCYB, NeurIPS, AAAI, IJCAI, CVPR, WWW, ACM MM,** etc., and received several research funds Youth Program of National Natural Science Foundation and General Program of Natural Science Foundation of Guangdong Province. 


# News
---

- **Sep, 2024**: A paper, “EGonc : Energy-based Open-Set Node Classification with substitute Unknowns”, is accepted by NeurIPS, 2024. Congratulations, Zelin!
- **Sep, 2024**: A paper, “CHAmbi: A New Benchmark on Chinese Ambiguity Challenges for Large Language Models”, is accepted by EMNLP, 2024. Congratulations, Sihan!
- **July, 2024**: A paper, “Open-world Structured Sequence Learning Via Dense Target Encoding”, is accepted by Information Sciences, 2024. Congratulations, Ziqi!
- **July, 2024**: A paper, “ReCoS: A Novel Benchmark for Cross-Modal Image-Text Retrieval in Complex Real-Life Scenarios”, is accepted by ACM MM, 2024. Congratulations, Jimeng!
- **July, 2024**: A paper, “A Payment Transaction Pre-training Model for Fraud Transaction Detection”, is accepted by CIKM, 2024. Congratulations, Wenxi!
- **Aug, 2024**: I will attend and serve as a session chair in IJCAI 2024 in Jeju Island, South Korea. Hope to see you there.
- **May, 2024**: A paper, “End-to-end approach of multi-grained embedding of categorical features in tabular data”, is accepted by IPM, 2024. Congratulations, Han Liu!
- **May, 2024**: A paper, “A fine-grained self-adapting prompt learning approach for few-shot learning with pre-trained language models”, is accepted by KBS, 2024. Congratulations, Xiaojun!
- **Apr, 2024**: A paper, “CONC: Complex-noise-resistant Open-set Node Classification with Adaptive Noise Detection”, is accepted by IJCAI, 2024. Congratulations, Jiexin!
- **May, 2024**: A paper, “PH-Net: Semi-Supervised Breast Lesion Segmentation via Patch-wise Hardness”, is accepted by CVPR, 2024. Congratulations, Siyao!
- **Dec, 2023**: Two papers, “ROGPL: Robust Open-Set Graph Learning via Region-based Prototype Learning” and “Multi-level Cross-modal Alignment for Image Clustering”, are accepted by AAAI 2024. Congratulations, Xiaowei and Liping!
- **Dec, 2023**: A paper, “Unsupervised multiple choices question answering via universal corpus”, is accepted by ICASSP, 2024. Congratulations, Hao Ge!

# Publications
---

- **EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models**
    <br>**Jingyuan Yang**, Jiawei Feng, and Hui Huang\*
    <br>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(**CVPR**), 6358-6368, 2024
    <br>\[[Project page](https://vcc.tech/research/2024/EmoGen)\]\[[Code](https://github.com/JingyuanYY/EmoGen)\]\[[PDF](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_EmoGen_Emotional_Image_Content_Generation_with_Text-to-Image_Diffusion_Models_CVPR_2024_paper.html)\]
    
    <div align="center">
    <img src="../assets/6.png" width="70%">
    </div>

- **EmoSet: A Large-scale Visual Emotion Dataset with Rich Attributes**
    <br>**Jingyuan Yang**, Qirui Huang, Tingting Ding, Dani Lischinski, Daniel Cohen-Or, and Hui Huang\*
    <br>Proceedings of the IEEE International Conference on Computer Vision (**ICCV**), 20383-20394, 2023
    <br>\[[Project page](https://vcc.tech/EmoSet)\]\[[Code](https://github.com/JingyuanYY/EmoSet)\]\[[PDF](https://openaccess.thecvf.com/content/ICCV2023/html/Yang_EmoSet_A_Large-scale_Visual_Emotion_Dataset_with_Rich_Attributes_ICCV_2023_paper.html)\]
    
    <div align="center">
    <img src="../assets/5.jpg" width="90%">
    </div>

- **Seeking Subjectivity in Visual Emotion Distribution Learning** 
    <br>**Jingyuan Yang**, Jie Li, Leida Li, Xiumei Wang, Yuxuan Ding, and Xinbo Gao\*
    <br>IEEE Transactions on Image Processing (**TIP**), 31, 5189-5202, 2022
    <br>[[Code](https://github.com/JingyuanYY/SAMNet)\]\[[PDF](https://ieeexplore.ieee.org/abstract/document/9846869)\]
    
    <div align="center">
    <img src="../assets/4.png" width="70%">
    </div>

- **A Circular-Structured Representation for Visual Emotion Distribution Learning** 
    <br>**Jingyuan Yang**, Jie Li, Leida Li, Xiumei Wang, and Xinbo Gao\*
    <br>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (**CVPR**), 4237-4246, 2021
    <br>[[Code](https://github.com/JingyuanYY/Circular-structured-representation)\]\[[PDF](https://openaccess.thecvf.com/content/CVPR2021/html/Yang_A_Circular-Structured_Representation_for_Visual_Emotion_Distribution_Learning_CVPR_2021_paper.html)\]
    
    <div align="center">
    <img src="../assets/3.png" width="70%">
    </div>

- **SOLVER: Scene-Object Interrelated Visual Emotion Reasoning Network** 
    <br>**Jingyuan Yang**, Xinbo Gao\*, Leida Li, Xiumei Wang, and Jinshan Ding
    <br>IEEE Transactions on Image Processing (**TIP**), 30, 8686-8701, 2021
    <br>[[Code](https://github.com/JingyuanYY/SOLVER)\]\[[PDF](https://ieeexplore.ieee.org/abstract/document/9580604)\]
    
    <div align="center">
    <img src="../assets/2.png" width="70%">
    </div>

- **Stimuli-Aware Visual Emotion Analysis** 
    <br>**Jingyuan Yang**, Jie Li, Xiumei Wang, Yuxuan Ding, and Xinbo Gao\*
    <br>IEEE Transactions on Image Processing (**TIP**), 30, 7432-7445, 2021
    <br>[[Code](https://github.com/JingyuanYY/Stimuli-aware-VEA)\]\[[PDF](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9524517)\]
    
    <div align="center">
    <img src="../assets/1.png" width="70%">
    </div>
    
# Fundings
---
<ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACM MM’24, Oral</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/freepih_mm24-480.webp 480w,/assets/img/publication_preview/freepih_mm24-800.webp 800w,/assets/img/publication_preview/freepih_mm24-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/freepih_mm24" class="preview z-depth-1 rounded medium-zoom-image" width="100%" height="auto" alt="freepih_mm24" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:journals/corr/abs-2311-14926" class="col-sm-8"> <div class="title">FreePIH: Training-Free Painterly Image Harmonization with Diffusion Model</div> <div class="author"> Ruibin Li,&nbsp;Jingcai Guo,&nbsp;<em>Qihua Zhou</em>,&nbsp;and&nbsp;Song Guo </div> <div class="periodical"> <em>In Proceedings of the ACM International Conference on Multimedia (ACM-MM, Oral, CCF-A)</em> , Melbourne, Australia, Jul 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/pdf?id=de7GoqU3Uv" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This paper provides an efficient training-free painterly image harmonization (PIH) method, dubbed FreePIH, that leverages only a pre-trained diffusion model to achieve state-of-the-art harmonization results. Unlike existing methods that require either training auxiliary networks or fine-tuning a large pre-trained backbone, or both, to harmonize a foreground object with a painterly-style background image, our FreePIH tames the denoising process as a plug-in module for foreground image style transfer. Specifically, we find that the very last few steps of the denoising (i.e., generation) process strongly correspond to the stylistic information of images, and based on this, we propose to augment the latent features of both the foreground and background images with Gaussians for a direct denoising-based harmonization. To guarantee the fidelity of the harmonized image, we make use of multi-scale features to enforce the consistency of the content and stability of the foreground objects in the latent space, and meanwhile, aligning both fore-/back-grounds with the same style. Moreover, to accommodate the generation with more structural and textural details, we further integrate text prompts to attend to the latent features, hence improving the generation quality. Quantitative and qualitative evaluations on COCO and LAION 5B datasets demonstrate that our method can surpass representative baselines by large margins.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><div class="code-display-wrapper"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:journals/corr/abs-2311-14926</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Ruibin and Guo, Jingcai and Zhou, Qihua and Guo, Song}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FreePIH: Training-Free Painterly Image Harmonization with Diffusion
                    Model}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the ACM International Conference on Multimedia (ACM-MM, Oral, CCF-A)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Melbourne, Australia}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.48550/arXiv.2311.14926}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2311.14926}</span><span class="p">,</span>
  <span class="na">eprinttype</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2311.14926}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Wed, 17 Jul 2024 16:21:24 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/journals/corr/abs-2311-14926.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre><button class="copy" type="button" aria-label="Copy code to clipboard"><i class="fa-solid fa-clipboard"></i></button></div></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IJCAI’24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/parsnets_ijcai24-480.webp 480w,/assets/img/publication_preview/parsnets_ijcai24-800.webp 800w,/assets/img/publication_preview/parsnets_ijcai24-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/parsnets_ijcai24" class="preview z-depth-1 rounded medium-zoom-image" width="100%" height="auto" alt="parsnets_ijcai24" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:journals/corr/abs-2312-09709" class="col-sm-8"> <div class="title">ParsNets: A Parsimonious Orthogonal and Low-Rank Linear Networks for Zero-Shot Learning</div> <div class="author"> Jingcai Guo,&nbsp;<em>Qihua Zhou</em>,&nbsp;Ruibing Li,&nbsp;Xiaocheng Lu,&nbsp;Ziming Liu,&nbsp;Junyang Chen,&nbsp;Xin Xie,&nbsp;and&nbsp;Jie Zhang </div> <div class="periodical"> <em>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI, CCF-A)</em> , Jeju, Apr 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2312.09709" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This paper provides a novel parsimonious yet efficient design for zero-shot learning (ZSL), dubbed ParsNets, in which we are interested in learning a composition of on-device friendly linear networks, each with orthogonality and low-rankness properties, to achieve equivalent or better performance against deep models. Concretely, we first refactor the core module of ZSL, i.e., the visual-semantics mapping function, into several base linear networks that correspond to diverse components of the semantic space, wherein the complex nonlinearity can be collapsed into simple local linearities. Then, to facilitate the generalization of local linearities, we construct a maximal margin geometry on the learned features by enforcing low-rank constraints on intra-class samples and high-rank constraints on inter-class samples, resulting in orthogonal subspaces for different classes. To enhance the model’s adaptability and counterbalance the over-/under-fittings, a set of sample-wise indicators is employed to select a sparse subset from these base linear networks to form a composite semantic predictor for each sample. Notably, maximal margin geometry can guarantee the diversity of features and, meanwhile, local linearities guarantee efficiency. Thus, our ParsNets can generalize better to unseen classes and can be deployed flexibly on resource-constrained devices.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><div class="code-display-wrapper"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:journals/corr/abs-2312-09709</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo, Jingcai and Zhou, Qihua and Li, Ruibing and Lu, Xiaocheng and Liu, Ziming and Chen, Junyang and Xie, Xin and Zhang, Jie}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ParsNets: {A} Parsimonious Orthogonal and Low-Rank Linear Networks
                    for Zero-Shot Learning}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI, CCF-A)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Jeju}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.48550/arXiv.2312.09709}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2312.09709}</span><span class="p">,</span>
  <span class="na">eprinttype</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2312.09709}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Tue, 09 Jan 2024 12:22:59 +0100}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/journals/corr/abs-2312-09709.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre><button class="copy" type="button" aria-label="Copy code to clipboard"><i class="fa-solid fa-clipboard"></i></button></div></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE TPAMI’24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pass_tpami24-480.webp 480w,/assets/img/publication_preview/pass_tpami24-800.webp 800w,/assets/img/publication_preview/pass_tpami24-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/pass_tpami24" class="preview z-depth-1 rounded medium-zoom-image" width="100%" height="auto" alt="pass_tpami24" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:journals/pami/ZhouGPLGXZ24" class="col-sm-8"> <div class="title">PASS: Patch Automatic Skip Scheme for Efficient On-Device Video Perception</div> <div class="author"> <em>Qihua Zhou</em>,&nbsp;Song Guo,&nbsp;Jun Pan,&nbsp;Jiacheng Liang,&nbsp;Jingcai Guo,&nbsp;Zhenda Xu,&nbsp;and&nbsp;Jingren Zhou </div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI, CCF-A, IF=23.6)</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10381763" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Real-time video perception tasks are often challenging on resource-constrained edge devices due to the issues of accuracy drop and hardware overhead, where saving computations is the key to performance improvement. Existing methods either rely on domain-specific neural chips or priorly searched models, which require specialized optimization according to different task properties. These limitations motivate us to design a general and task-independent methodology, called Patch Automatic Skip Scheme (PASS), which supports diverse video perception settings by decoupling acceleration and tasks. The gist is to capture inter-frame correlations and skip redundant computations at patch level, where the patch is a non-overlapping square block in visual. PASS equips each convolution layer with a learnable gate to selectively determine which patches could be safely skipped without degrading model accuracy. Specifically, we are the first to construct a self-supervisory procedure for gate optimization, which learns to extract contrastive representations from frame sequences. The pre-trained gates can serve as plug-and-play modules to implement patch-skippable neural backbones, and automatically generate proper skip strategy to accelerate different video-based downstream tasks, e.g., outperforming state-of-the-art MobileHumanPose in 3D pose estimation and FairMOT in multiple object tracking, by up to 9.43× and 12.19× speedups, respectively, on NVIDIA Jetson Nano devices.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><div class="code-display-wrapper"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/pami/ZhouGPLGXZ24</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Guo, Song and Pan, Jun and Liang, Jiacheng and Guo, Jingcai and Xu, Zhenda and Zhou, Jingren}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{PASS:} Patch Automatic Skip Scheme for Efficient On-Device Video
                    Perception}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{IEEE} Transactions on Pattern Analysis and Machine Intelligence (TPAMI, CCF-A, IF=23.6)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{46}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3938--3954}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/TPAMI.2024.3350380}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TPAMI.2024.3350380}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Sat, 04 May 2024 10:55:20 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/journals/pami/ZhouGPLGXZ24.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre><button class="copy" type="button" aria-label="Copy code to clipboard"><i class="fa-solid fa-clipboard"></i></button></div></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE TMC’24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tc_tmc24-480.webp 480w,/assets/img/publication_preview/tc_tmc24-800.webp 800w,/assets/img/publication_preview/tc_tmc24-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/tc_tmc24" class="preview z-depth-1 rounded medium-zoom-image" width="100%" height="auto" alt="tc_tmc24" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10382540" class="col-sm-8"> <div class="title">Chiron: A Robustness-Aware Incentive Scheme for Edge Learning Via Hierarchical Reinforcement Learning</div> <div class="author"> Yi Liu,&nbsp;Song Guo,&nbsp;Yufeng Zhan,&nbsp;Leijie Wu,&nbsp;Zicong Hong,&nbsp;and&nbsp;<em>Qihua Zhou</em> </div> <div class="periodical"> <em>IEEE Transactions on Mobile Computing (TMC, CCF-A, IF=7.9)</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10382540" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Over the past few years, edge learning has achieved significant success in mobile edge networks. Few works have designed incentive mechanism that motivates edge nodes to participate in edge learning. However, most existing works only consider myopic optimization and assume that all edge nodes are honest, which lacks long-term sustainability and the final performance assurance. In this paper, we propose Chiron, an incentive-driven Byzantine-resistant long-term mechanism based on hierarchical reinforcement learning (HRL). First, our optimization goal includes both learning-algorithm performance criteria (i.e., global accuracy) and systematical criteria (i.e., resource consumption), which aim to improve the edge learning performance under a given resource budget. Second, we propose a three-layer HRL architecture to handle long-term optimization, short-term optimization, and byzantine resistance, respectively. Finally, we conduct experiments on various edge learning tasks to demonstrate the superiority of the proposed approach. Specifically, our system can successfully exclude malicious nodes and lazy nodes out of the edge learning participation and achieves 14.96% higher accuracy and 12.66% higher total utility than the state-of-the-art methods under the same budget limit.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><div class="code-display-wrapper"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10382540</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Yi and Guo, Song and Zhan, Yufeng and Wu, Leijie and Hong, Zicong and Zhou, Qihua}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Mobile Computing (TMC, CCF-A, IF=7.9)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Chiron: A Robustness-Aware Incentive Scheme for Edge Learning Via Hierarchical Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-17}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Training;Servers;Optimization;Deep learning;Computational modeling;Reinforcement learning;Data models;Deep reinforcement learning;edge learning;incentive mechanism;mobile edge computing}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TMC.2024.3350654}</span>
<span class="p">}</span></code></pre><button class="copy" type="button" aria-label="Copy code to clipboard"><i class="fa-solid fa-clipboard"></i></button></div></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI’24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/nevs_aaai24-480.webp 480w,/assets/img/publication_preview/nevs_aaai24-800.webp 800w,/assets/img/publication_preview/nevs_aaai24-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/nevs_aaai24" class="preview z-depth-1 rounded medium-zoom-image" width="100%" height="auto" alt="nevs_aaai24" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:conf/aaai/ZhouGGLZWX24" class="col-sm-8"> <div class="title">On the Robustness of Neural-Enhanced Video Streaming against Adversarial Attacks</div> <div class="author"> <em>Qihua Zhou</em>,&nbsp;Jingcai Guo,&nbsp;Song Guo,&nbsp;Ruibin Li,&nbsp;Jie Zhang,&nbsp;Bingjie Wang,&nbsp;and&nbsp;Zhenda Xu </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI, CCF-A)</em> , Vancouver, Canada, Feb 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29657" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The explosive growth of video traffic on today’s Internet promotes the rise of Neural-enhanced Video Streaming (NeVS), which effectively improves the rate-distortion trade-off by employing a cheap neural super-resolution model for quality enhancement on the receiver side. Missing by existing work, we reveal that the NeVS pipeline may suffer from a practical threat, where the crucial codec component (i.e., encoder for compression and decoder for restoration) can trigger adversarial attacks in a man-in-the-middle manner to significantly destroy video recovery performance and finally incurs the malfunction of downstream video perception tasks. In this paper, we are the first attempt to inspect the vulnerability of NeVS and discover a novel adversarial attack, called codec hijacking, where the injected invisible perturbation conspires with the malicious encoding matrix by reorganizing the spatial-temporal bit allocation within the bitstream size budget. Such a zero-day vulnerability makes our attack hard to defend because there is no visual distortion on the recovered videos until the attack happens. More seriously, this attack can be extended to diverse enhancement models, thus exposing a wide range of video perception tasks under threat. Evaluation based on state-of-the-art video codec benchmark illustrates that our attack significantly degrades the recovery performance of NeVS over previous attack methods. The damaged video quality finally leads to obvious malfunction of downstream tasks with over 75% success rate. We hope to arouse public attention on codec hijacking and its defence.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><div class="code-display-wrapper"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/aaai/ZhouGGLZWX24</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Guo, Jingcai and Guo, Song and Li, Ruibin and Zhang, Jie and Wang, Bingjie and Xu, Zhenda}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Wooldridge, Michael J. and Dy, Jennifer G. and Natarajan, Sriraam}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the Robustness of Neural-Enhanced Video Streaming against Adversarial
                    Attacks}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the {AAAI} Conference on Artificial Intelligence (AAAI, CCF-A)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{17123--17131}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{AAAI} Press}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Vancouver, Canada}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1609/aaai.v38i15.29657}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/AAAI.V38I15.29657}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Tue, 02 Apr 2024 16:32:09 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/conf/aaai/ZhouGGLZWX24.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre><button class="copy" type="button" aria-label="Copy code to clipboard"><i class="fa-solid fa-clipboard"></i></button></div></figure> </div> </div> </div> </li> </ol>
    

# Selected Awards
---

- **Graphic Open Source Dataset Award**, by CCF CAD&CG, 2024
- **Best Poster Award**, by CSIG CEI, 2023
- **Outstanding Graduate of Shaanxi Province**, by Education Department of Shaanxi Provincial Government, 2022
- **China National Scholarship**, by Ministry of Education of the People's Republic of China, 2015 & 2021
- **Speaking as the only student representative at the 90th Anniversary Celebration of Xidian University**, 2021

# Hobbies
---

- **English Speech** (First place in the Northwest Region Postgraduate English Speech Contest)
- **Badminton** (Women's singles champion in the Freshmen Cup at Xidian Univeristy)
- **Piano** (Amateur six level certificate of piano)

# Group
---
<center>
  <h3 style="margin-bottom: 10px;">&#128588; Welcome to join our Emotion Group at VCC! &#128588;</h3>
</center>

<div style="display: flex; justify-content: space-between; align-items: top; flex-wrap: wrap;">
  <div style="text-align: center; margin-bottom: 10px; display: inline-block; margin-right: 10px;">
    <img src="../assets/01.jpg" alt="图片1" style="width: 160px; height: 160px; border-radius: 50%;">
    <div style="text-align: center; font-size: 12px;">
      <h3 style="margin: 2px 0; font-size: 18px">Tingting Ding</h3>
      <p style="margin: 0px 0;">Ph.D., 2022-present</p>
      <p style="margin: 0px 0;">Co-advised by Prof. Huang</p>
    </div>
  </div>

  <div style="text-align: center; margin-bottom: 10px; display: inline-block; margin-right: 10px;">
    <img src="../assets/02.jpg" alt="图片2" style="width: 160px; height: 160px; border-radius: 50%;">
    <div style="text-align: center; font-size: 12px;">
      <h3 style="margin: 2px 0; font-size: 18px">Jiasheng Chen</h3>
      <p style="margin: 0px 0;">Master, 2022-present</p>
      <p style="margin: 0px 0;">Co-advised by Prof. Huang</p>
    </div>
  </div>

  <div style="text-align: center; margin-bottom: 10px; display: inline-block; margin-right: 10px;">
    <img src="../assets/04.jpg" alt="图片3" style="width: 160px; height: 160px; border-radius: 50%;">
    <div style="text-align: center; font-size: 12px;">
      <h3 style="margin: 2px 0; font-size: 18px">Jiawei Feng</h3>
      <p style="margin: 0px 0;">Master, 2023-present</p>
    </div>
  </div>
  
  <div style="text-align: center; margin-bottom: 10px; display: inline-block; margin-right: 10px;">
    <img src="../assets/05.jpg" alt="图片4" style="width: 160px; height: 160px; border-radius: 50%;">
    <div style="text-align: center; font-size: 12px;">
      <h3 style="margin: 2px 0; font-size: 18px">Weibin Luo</h3>
      <p style="margin: 0px 0;">Master, 2024-present</p>
    </div>
  </div>

  <div style="text-align: center; margin-bottom: 10px; display: inline-block; margin-right: 10px;">
    <img src="../assets/06.jpg" alt="图片5" style="width: 160px; height: 160px; border-radius: 50%;">
    <div style="text-align: center; font-size: 12px;">
      <h3 style="margin: 2px 0; font-size: 18px">Zihuan Bai</h3>
      <p style="margin: 0px 0;">Master, 2024-present</p>
    </div>
  </div>

  <div style="text-align: center; margin-bottom: 10px; display: inline-block; margin-right: 10px;">
    <img src="../assets/07.jpg" alt="图片4" style="width: 160px; height: 160px; border-radius: 50%;">
    <div style="text-align: center; font-size: 12px;">
      <h3 style="margin: 2px 0; font-size: 18px">Rucong Chen</h3>
      <p style="margin: 0px 0;">Master, 2024-present</p>
    </div>
  </div>
</div>
